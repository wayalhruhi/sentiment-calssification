{"cells":[{"metadata":{"id":"oJNVLZ__L3wF","colab_type":"text"},"cell_type":"markdown","source":"> This kernel is based on the work of https://www.kaggle.com/thebrownviking20/bert-multiclass-classification"},{"metadata":{"id":"undPrqPuMItC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"0dac6538-42a3-4e78-f43d-1fcc7fefab98","trusted":true},"cell_type":"code","source":"import os\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ZHQCFgoJL3wG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":260},"outputId":"b54b725f-19a4-437e-efe8-89111ab7d51b"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport sys\nimport zipfile\nimport datetime\nprint(os.listdir(\"../input\"))\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\ntrain = pd.read_csv('../input/train_2kmZucJ.csv')\ntest = pd.read_csv('../input/test_oJQbWVk.csv')\ndf = train\n\nll = len(df['label'].unique())\ndf.head()\n\n# Any results you write to the current directory are saved as output.\n#     print(os.listdir(f\"../input/{d}\"))\n# print(os.listdir(\"../input/googles-bert-model/chinese_l-12_h-768_a-12/chinese_L-12_H-768_A-12\"))\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"id":"9P8ZvIuzMYLi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"fa69d859-f1d4-4a22-c3f2-8839894b85e2","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"9GXz0XQwMYKV","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"bN8tM9IGMYHV","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"cBmlsHVPL3wK","colab_type":"text"},"cell_type":"markdown","source":"# 1. Kernel Overview"},{"metadata":{"id":"L9-AcdcJL3wL","colab_type":"text"},"cell_type":"markdown","source":"## 1.1 Defination :"},{"metadata":{"id":"RqX87FnWL3wM","colab_type":"text"},"cell_type":"markdown","source":"In today world** Text Classification/Segmentation/Categorization** (for example ticket categorization in a call centre, email classification, logs category detection etc.) is a common task. With humongous data out there, its nearly impossible to do this manually. Let's try to solve this problem automatically using machine learning and natural language processing tools."},{"metadata":{"id":"g3p6F7poL3wN","colab_type":"text"},"cell_type":"markdown","source":"## 1.2 Problem Statement"},{"metadata":{"id":"D_KBPSmqL3wN","colab_type":"text"},"cell_type":"markdown","source":"BBC articles dataset(2126 records) consist of two features text and the assiciated categories namely \n1. Sport \n2. Business \n3. Politics \n4. Tech \n5. Others\n\n**Our task is to train a multiclass classification model on the mentioned dataset.**"},{"metadata":{"id":"VqjHTVKSL3wO","colab_type":"text"},"cell_type":"markdown","source":"## 1.3 Metrics"},{"metadata":{"id":"yRtC_HK_L3wP","colab_type":"text"},"cell_type":"markdown","source":"**Accuracy** - Classification accuracy is the number of correct predictions made as a\nratio of all predictions made\n\n**Precision** - precision (also called positive predictive value) is the fraction of\nrelevant instances among the retrieved instances\n\n**F1_score** - considers both the precision and the recall of the test to compute the\nscore\n\n**Recall** â€“ recall (also known as sensitivity) is the fraction of relevant instances that\nhave been retrieved over the total amount of relevant instances\n\n**Why these metrics?** - We took Accuracy, Precision, F1 Score and Recall as metrics\nfor evaluating our model because accuracy would give an estimate of correct prediction. Precision would give us an estimate about the positive category predicted value i.e. how much our model is giving relevant result. F1 Score gives a clubbed estimate of precision and recall.Recall would provide us the relevant positive category prediction to the false negative and true positive category recognition results."},{"metadata":{"id":"OxAgxICRL3wQ","colab_type":"text"},"cell_type":"markdown","source":"## 1.4 Machine Learning Model Considered:"},{"metadata":{"id":"WYQJ9QCUL3wQ","colab_type":"text"},"cell_type":"markdown","source":"We will be using **BERT Base uncased model** for this use case. \n\nBert model working is not in the scope of this kernal. Kindly refer other external sources."},{"metadata":{"id":"wvWtR_0fL3wR","colab_type":"text"},"cell_type":"markdown","source":"Download Bert Requirements"},{"metadata":{"trusted":true,"id":"lZ4GbgcwL3wS","colab_type":"code","colab":{}},"cell_type":"code","source":"# !wget https://github.com/charles9n/bert-sklearn.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"A4ZrpYb6L3wV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c9504fdb-582f-4370-a40a-84ae50730322"},"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py ","execution_count":null,"outputs":[]},{"metadata":{"id":"O8n1YJXUL3wX","colab_type":"text"},"cell_type":"markdown","source":"# 2. Data Exploration"},{"metadata":{"id":"4BLOrpNeL3wY","colab_type":"text"},"cell_type":"markdown","source":"### Step 2.1 Load Dataset"},{"metadata":{"trusted":true,"id":"HyMMFmiVL3wZ","colab_type":"code","colab":{}},"cell_type":"code","source":"import pandas as pd\n\ndata=df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Ndj4iSrIL3wb","colab_type":"code","colab":{}},"cell_type":"code","source":"df.columns = ['category', 'text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"7fiPgnGrL3we","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"9e6d6050-e977-4451-f6ab-84946bd23094"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"sfzpjDThL3wj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"d0779324-67d2-4826-aeb3-07b1a20d31b8"},"cell_type":"code","source":"data['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"LrrYQG2UL3wm","colab_type":"text"},"cell_type":"markdown","source":"# 3. Implementation"},{"metadata":{"id":"NMBMO0IeL3wn","colab_type":"text"},"cell_type":"markdown","source":"### Step 2.2 Map Textual labels to numeric using Label Encoder"},{"metadata":{"trusted":true,"id":"HTCeF5oKL3wo","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndf2 = pd.DataFrame()\ndf2[\"text\"] = data[\"text\"]\ndf2[\"label\"] = (data[\"category\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"gBL3EYEhL3wq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"74ff7175-887c-4080-8f52-9f980d315998"},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"vehTSy04L3ws","colab_type":"text"},"cell_type":"markdown","source":"### Step 2.3 Divide dataset to test and train dataset"},{"metadata":{"trusted":true,"id":"RDDq4m7wL3wt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"e30fd03f-fe49-4492-bf9c-5436786c3c0f"},"cell_type":"code","source":"test.columns = ['text']\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"EX6eATlUL3ww","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(df2[\"text\"].values, df2[\"label\"].values, test_size=0.0002, random_state=42) ;X_test = test['text'].values\nX_train, X_test, y_train, y_test = train_test_split(df2[\"text\"].values, df2[\"label\"].values, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"Lki-KbTjL3wy","colab_type":"text"},"cell_type":"markdown","source":"### Step 2.4 Setting up BERT Configurations"},{"metadata":{"trusted":true,"id":"wnpnJj4gL3wz","colab_type":"code","colab":{}},"cell_type":"code","source":"import modeling\nimport optimization\nimport run_classifier\nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"KyuHtdopL3w4","colab_type":"code","colab":{}},"cell_type":"code","source":"import zipfile\n\nfolder = 'model_folder'\nwith zipfile.ZipFile(\"cased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(folder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"V02xJB9vL3w7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"6bb7fca8-a182-4ad8-bd42-ec231dcde24d"},"cell_type":"code","source":"BERT_MODEL = 'cased_L-12_H-768_A-12.zip'\nBERT_PRETRAINED_DIR = f'{folder}/cased_L-12_H-768_A-12'\nOUTPUT_DIR = f'{folder}/outputs'\nprint(f'>> Model output directory: {OUTPUT_DIR}')\nprint(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Q8JCjEqML3w-","colab_type":"code","colab":{}},"cell_type":"code","source":"# label_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"EZ_GWqKuL3xD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"90143607-ab6e-4ad6-f921-6688c20d2505"},"cell_type":"code","source":"import os\nimport tensorflow as tf\n\ndef create_examples(lines, set_type, labels=None):\n#Generate data for the BERT model\n    guid = f'{set_type}'\n    examples = []\n    if guid == 'train':\n        for line, label in zip(lines, labels):\n            text_a = line\n            label = str(label)\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    else:\n        for line in lines:\n            text_a = line\n            label = '0'\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n# Model Hyper Parameters\nTRAIN_BATCH_SIZE = 32\nEVAL_BATCH_SIZE = 4\nLEARNING_RATE = 1e-5\nNUM_TRAIN_EPOCHS = 8.0\nWARMUP_PROPORTION = 0.1\nMAX_SEQ_LENGTH = 150\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 100000 #if you wish to finetune a model on a larger dataset, use larger interval\n# each checpoint weights about 1,5gb\nITERATIONS_PER_LOOP = 100000\nNUM_TPU_CORES = 8\nVOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\nINIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nDO_LOWER_CASE = BERT_MODEL.startswith('cased')\n\nlabel_list = [str(num) for num in range(ll) ]\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\ntrain_examples = create_examples(X_train, 'train', labels=y_train)\n\ntpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=ITERATIONS_PER_LOOP,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n\nnum_train_steps = int(\n    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\nmodel_fn = run_classifier.model_fn_builder(\n    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n    num_labels=len(label_list),\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n    use_one_hot_embeddings=True)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"id":"DbwEduBGL3xM","colab_type":"text"},"cell_type":"markdown","source":"### Step 2.5 Train BERT model"},{"metadata":{"trusted":true,"id":"XRGLD6qFL3xN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"03564612-c3c4-4705-ea17-776bde3ba119"},"cell_type":"code","source":"%%time\nimport datetime\n\nprint('Please wait...')\ntrain_features = run_classifier.convert_examples_to_features(\n    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\nprint('>> Started training at {} '.format(datetime.datetime.now()))\nprint('  Num examples = {}'.format(len(train_examples)))\nprint('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\ntf.logging.info(\"  Num steps = %d\", num_train_steps)\ntrain_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=True)\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint('>> Finished training at {}'.format(datetime.datetime.now()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"r3JvPSxDL3xS","colab_type":"code","colab":{}},"cell_type":"code","source":"def input_fn_builder(features, seq_length, is_training, drop_remainder):\n    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n    \n    all_input_ids = []\n    all_input_mask = []\n    all_segment_ids = []\n    all_label_ids = []\n\n    for feature in features:\n        all_input_ids.append(feature.input_ids)\n        all_input_mask.append(feature.input_mask)\n        all_segment_ids.append(feature.segment_ids)\n        all_label_ids.append(feature.label_id)\n\n    def input_fn(params):\n        \"\"\"The actual input function.\"\"\"\n        print(params)\n        batch_size = 500\n\n        num_examples = len(features)\n\n        d = tf.data.Dataset.from_tensor_slices({\n            \"input_ids\":\n                tf.constant(\n                    all_input_ids, shape=[num_examples, seq_length],\n                    dtype=tf.int32),\n            \"input_mask\":\n                tf.constant(\n                    all_input_mask,\n                    shape=[num_examples, seq_length],\n                    dtype=tf.int32),\n            \"segment_ids\":\n                tf.constant(\n                    all_segment_ids,\n                    shape=[num_examples, seq_length],\n                    dtype=tf.int32),\n            \"label_ids\":\n                tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n        })\n\n        if is_training:\n            d = d.repeat()\n            d = d.shuffle(buffer_size=100)\n\n        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n        return d\n    return input_fn","execution_count":null,"outputs":[]},{"metadata":{"id":"txSUtx-SL3xU","colab_type":"text"},"cell_type":"markdown","source":"### Step 2.6 Prediction on Test Dataset"},{"metadata":{"trusted":true,"id":"Rm0DojBNL3xV","colab_type":"code","colab":{}},"cell_type":"code","source":"\npredict_examples = create_examples(X_test, 'test')\n\npredict_features = run_classifier.convert_examples_to_features(\n    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\npredict_input_fn = input_fn_builder(\n    features=predict_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)\n\nresult = estimator.predict(input_fn=predict_input_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"XAGRJGVcL3xZ","colab_type":"code","colab":{}},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"N-R6qQ2aL3xc","colab_type":"code","colab":{}},"cell_type":"code","source":"# from xgboost import XGBClassifier\n# from lightgbm import LGBMClassifier\n# xg = LGBMClassifier()\n# xg.fit(train_features, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"VTxRkOVKL3xe","colab_type":"code","colab":{}},"cell_type":"code","source":"# import gc\n# gc.enable()\n# gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"WHEMRaeYL3xg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"c43728e1-ae72-47e1-fcb7-d192f6ae967d"},"cell_type":"code","source":"import numpy as np\npreds = []\nfor prediction in result:\n      preds.append(np.argmax(prediction['probabilities']))\nprint(len(preds))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"A0FRTc8rL3xh","colab_type":"text"},"cell_type":"markdown","source":"# 4. Results"},{"metadata":{"trusted":true,"id":"CpAp4qe2L3xi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"aefacf1c-67ca-4407-a6fa-e24266062259"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nprint(\"Accuracy of BERT is:\",f1_score(y_test,preds, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"tzHJHGHwL3xl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":179},"outputId":"fde4ad6c-fc55-4ed1-9bc7-9b707a200eb9"},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test,preds))","execution_count":null,"outputs":[]},{"metadata":{"id":"HpEeggAFL3xo","colab_type":"text"},"cell_type":"markdown","source":">** Past Work mentioned on this dataset at max achieved 95.22 accuracies. BERT base model for the same, without any preprocessing and achieved 97.75 accuracies.**"},{"metadata":{"trusted":true,"id":"fkDXUMXuL3xp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"9b1b66d1-33f6-466d-f639-7cb03049e00d"},"cell_type":"code","source":"len(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"bVpW63bCL3xt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":409},"outputId":"feb7dcfd-5bec-4ea4-96d7-e1f463d60578"},"cell_type":"code","source":"sam = pd.read_csv('../input/sample_submission_LnhVWA4.csv')\nsam.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"tg0VYTyZL3xv","colab_type":"code","colab":{}},"cell_type":"code","source":"sam['label'] = preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"nI2i3vMaL3xx","colab_type":"code","colab":{}},"cell_type":"code","source":"sam.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"3b0_lPwJL3x2","colab_type":"code","colab":{}},"cell_type":"code","source":"sam.to_csv('sub.csv', index=False)\nfrom IPython.display import FileLinks\nFileLinks('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"_2PNE_o0L3x4","colab_type":"code","colab":{}},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"qNHQk_TqL3x5","colab_type":"code","colab":{}},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"xBsVWPiGL3x7","colab_type":"text"},"cell_type":"markdown","source":"# 5. Future Improvements on this kernel:"},{"metadata":{"id":"LToRJudGL3x8","colab_type":"text"},"cell_type":"markdown","source":"* Explore preprocessing steps on data.\n* Explore other models as baseline.\n* Make this notebook more informative and illustrative.\n* Explaination on Bert Model.\n* More time on data exploration\nand many more..."},{"metadata":{"id":"7VNS0V3yL3x8","colab_type":"text"},"cell_type":"markdown","source":"# 6. References"},{"metadata":{"id":"_TgBd_pKL3x9","colab_type":"text"},"cell_type":"markdown","source":"https://www.kaggle.com/thebrownviking20/bert-multiclass-classification"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"bert small cased ==54.44 (1).ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}
